{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is only for testing. Maybe we can build a working model from this, and then we can start putting code into the \"model\" python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../py')\n",
    "import cavitylearn.data\n",
    "import cavitylearn.catalonet0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_basedir = '/var/cavitylearn/dataset-test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataconfig = cavitylearn.data.DataConfig(classes=[\"SO4\",\"GOL\",\"EDO\",\"HEM\",\"NAG\",\"PO4\",\"ACT\"], num_props=8, boxshape=[30, 30, 30])\n",
    "with open(os.path.join(dataset_basedir, \"labels.txt\"), 'rt') as labelfile:\n",
    "    trainset = cavitylearn.data.DataSet(labelfile, os.path.join(dataset_basedir, \"boxes\"), dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Examples available in dataset: ', trainset.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxes_placeholder = tf.placeholder(tf.float32, shape=[None, dataconfig.boxshape[0], dataconfig.boxshape[1], dataconfig.boxshape[2], dataconfig.num_props])\n",
    "labels_placholder = tf.placeholder(tf.float32, shape=[None, dataconfig.num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = cavitylearn.catalonet0.inference(boxes_placeholder, dataconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = cavitylearn.catalonet0.loss(logits, labels_placholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_op = cavitylearn.catalonet0.train(loss, 1e-4, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits,1),  tf.argmax(labels_placholder,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "tf.scalar_summary(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purge(directory, pattern):\n",
    "    for f in os.listdir(directory):\n",
    "        if re.search(pattern, f):\n",
    "            os.remove(os.path.join(directory, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTINUE = True\n",
    "BATCHSIZE = 50\n",
    "NUM_BATCHES = 10\n",
    "RUN_NAME = \"run0\"\n",
    "CHECKPOINT_FREQUENCY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(dataset_basedir, \"checkpoints\", RUN_NAME)\n",
    "trainset.rewind_batches()\n",
    "with tf.Session() as sess:\n",
    "    # purge log directory for run, before running it again\n",
    "    purge(os.path.join(dataset_basedir, \"logs\", RUN_NAME), r'^events\\.out\\.tfevents\\.\\d+')\n",
    "    \n",
    "    # Create summary writer\n",
    "    summary_writer = tf.train.SummaryWriter(os.path.join(dataset_basedir, \"logs\", RUN_NAME), sess.graph)\n",
    "    \n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(NUM_BATCHES):\n",
    "        # get training data\n",
    "        labels, boxes = trainset.next_batch(BATCHSIZE)\n",
    "        \n",
    "        # abort training if we didn't get any more data\n",
    "        if len(labels) == 0:\n",
    "            break\n",
    "        \n",
    "        feed_dict = {\n",
    "            boxes_placeholder: boxes,\n",
    "            labels_placholder: labels\n",
    "        }\n",
    "        \n",
    "        # Do it!\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        print('.', end=\"\")\n",
    "        \n",
    "        \n",
    "        summary_str, step = sess.run([summary_op, global_step], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "        \n",
    "        if i % CHECKPOINT_FREQUENCY == 0:\n",
    "            saver.save(sess, checkpoint_dir, global_step=global_step)\n",
    "            \n",
    "        \n",
    "    saver.save(sess, checkpoint_dir, global_step=global_step)\n",
    "    end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Total time: \", str(end_time - start_time))\n",
    "print(\"Time per batch: \", str((end_time - start_time)/NUM_BATCHES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "importlib.reload(cavitylearn.catalonet0)\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
